{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 921,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010860711376595167,
      "grad_norm": 12.143192291259766,
      "learning_rate": 9e-07,
      "loss": 3.1095,
      "step": 10
    },
    {
      "epoch": 0.021721422753190334,
      "grad_norm": 751.58447265625,
      "learning_rate": 1.9e-06,
      "loss": 3.1045,
      "step": 20
    },
    {
      "epoch": 0.0325821341297855,
      "grad_norm": 10.574647903442383,
      "learning_rate": 2.9e-06,
      "loss": 3.0234,
      "step": 30
    },
    {
      "epoch": 0.04344284550638067,
      "grad_norm": 10.131420135498047,
      "learning_rate": 3.9e-06,
      "loss": 2.9249,
      "step": 40
    },
    {
      "epoch": 0.054303556882975834,
      "grad_norm": 25.29486656188965,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 2.7695,
      "step": 50
    },
    {
      "epoch": 0.065164268259571,
      "grad_norm": 10.584147453308105,
      "learning_rate": 5.9e-06,
      "loss": 2.9656,
      "step": 60
    },
    {
      "epoch": 0.07602497963616617,
      "grad_norm": 15.744433403015137,
      "learning_rate": 6.900000000000001e-06,
      "loss": 2.6093,
      "step": 70
    },
    {
      "epoch": 0.08688569101276133,
      "grad_norm": 13.985655784606934,
      "learning_rate": 7.9e-06,
      "loss": 2.6609,
      "step": 80
    },
    {
      "epoch": 0.0977464023893565,
      "grad_norm": 18.23666763305664,
      "learning_rate": 8.9e-06,
      "loss": 2.4954,
      "step": 90
    },
    {
      "epoch": 0.10860711376595167,
      "grad_norm": 7.674300670623779,
      "learning_rate": 9.900000000000002e-06,
      "loss": 2.3018,
      "step": 100
    },
    {
      "epoch": 0.11946782514254684,
      "grad_norm": 7.597133159637451,
      "learning_rate": 1.09e-05,
      "loss": 2.2336,
      "step": 110
    },
    {
      "epoch": 0.130328536519142,
      "grad_norm": 7.307061195373535,
      "learning_rate": 1.19e-05,
      "loss": 2.105,
      "step": 120
    },
    {
      "epoch": 0.14118924789573717,
      "grad_norm": 8.432183265686035,
      "learning_rate": 1.29e-05,
      "loss": 2.147,
      "step": 130
    },
    {
      "epoch": 0.15204995927233234,
      "grad_norm": 7.230419635772705,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 2.2925,
      "step": 140
    },
    {
      "epoch": 0.1629106706489275,
      "grad_norm": 6.390376567840576,
      "learning_rate": 1.49e-05,
      "loss": 2.0921,
      "step": 150
    },
    {
      "epoch": 0.17377138202552267,
      "grad_norm": 5.243844985961914,
      "learning_rate": 1.59e-05,
      "loss": 2.1816,
      "step": 160
    },
    {
      "epoch": 0.18463209340211784,
      "grad_norm": 6.061484336853027,
      "learning_rate": 1.69e-05,
      "loss": 2.0182,
      "step": 170
    },
    {
      "epoch": 0.195492804778713,
      "grad_norm": 10.813125610351562,
      "learning_rate": 1.79e-05,
      "loss": 1.9978,
      "step": 180
    },
    {
      "epoch": 0.20635351615530817,
      "grad_norm": 4.974493026733398,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 1.8561,
      "step": 190
    },
    {
      "epoch": 0.21721422753190334,
      "grad_norm": 10.3255033493042,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 1.9187,
      "step": 200
    },
    {
      "epoch": 0.2280749389084985,
      "grad_norm": 5.679133892059326,
      "learning_rate": 2.09e-05,
      "loss": 1.8207,
      "step": 210
    },
    {
      "epoch": 0.23893565028509367,
      "grad_norm": 4.539823532104492,
      "learning_rate": 2.19e-05,
      "loss": 1.782,
      "step": 220
    },
    {
      "epoch": 0.24979636166168884,
      "grad_norm": 8.980951309204102,
      "learning_rate": 2.29e-05,
      "loss": 1.8694,
      "step": 230
    },
    {
      "epoch": 0.260657073038284,
      "grad_norm": 5.980117321014404,
      "learning_rate": 2.39e-05,
      "loss": 1.777,
      "step": 240
    },
    {
      "epoch": 0.27151778441487917,
      "grad_norm": 5.403964519500732,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 1.8254,
      "step": 250
    },
    {
      "epoch": 0.28237849579147434,
      "grad_norm": 4.545988082885742,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 1.7786,
      "step": 260
    },
    {
      "epoch": 0.2932392071680695,
      "grad_norm": 4.901678562164307,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 1.7314,
      "step": 270
    },
    {
      "epoch": 0.30409991854466467,
      "grad_norm": 5.199270725250244,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 1.7465,
      "step": 280
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 4.636674404144287,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 1.8356,
      "step": 290
    },
    {
      "epoch": 0.325821341297855,
      "grad_norm": 7.3815131187438965,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 1.6575,
      "step": 300
    },
    {
      "epoch": 0.33668205267445017,
      "grad_norm": 7.741734981536865,
      "learning_rate": 3.09e-05,
      "loss": 1.8602,
      "step": 310
    },
    {
      "epoch": 0.34754276405104534,
      "grad_norm": 10.91911792755127,
      "learning_rate": 3.19e-05,
      "loss": 1.8893,
      "step": 320
    },
    {
      "epoch": 0.3584034754276405,
      "grad_norm": 5.220031261444092,
      "learning_rate": 3.29e-05,
      "loss": 1.8153,
      "step": 330
    },
    {
      "epoch": 0.3692641868042357,
      "grad_norm": 5.0126495361328125,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 1.7557,
      "step": 340
    },
    {
      "epoch": 0.38012489818083084,
      "grad_norm": 3.676178455352783,
      "learning_rate": 3.49e-05,
      "loss": 1.729,
      "step": 350
    },
    {
      "epoch": 0.390985609557426,
      "grad_norm": 4.337136268615723,
      "learning_rate": 3.59e-05,
      "loss": 1.6526,
      "step": 360
    },
    {
      "epoch": 0.4018463209340212,
      "grad_norm": 5.899774551391602,
      "learning_rate": 3.69e-05,
      "loss": 1.7125,
      "step": 370
    },
    {
      "epoch": 0.41270703231061634,
      "grad_norm": 4.508466720581055,
      "learning_rate": 3.79e-05,
      "loss": 1.6886,
      "step": 380
    },
    {
      "epoch": 0.4235677436872115,
      "grad_norm": 4.718142986297607,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 1.7073,
      "step": 390
    },
    {
      "epoch": 0.4344284550638067,
      "grad_norm": 5.768546104431152,
      "learning_rate": 3.99e-05,
      "loss": 1.7233,
      "step": 400
    },
    {
      "epoch": 0.44528916644040184,
      "grad_norm": 13.26321029663086,
      "learning_rate": 4.09e-05,
      "loss": 1.7024,
      "step": 410
    },
    {
      "epoch": 0.456149877816997,
      "grad_norm": 4.701363563537598,
      "learning_rate": 4.19e-05,
      "loss": 1.6102,
      "step": 420
    },
    {
      "epoch": 0.4670105891935922,
      "grad_norm": 6.5753679275512695,
      "learning_rate": 4.29e-05,
      "loss": 1.7617,
      "step": 430
    },
    {
      "epoch": 0.47787130057018734,
      "grad_norm": 4.58049201965332,
      "learning_rate": 4.39e-05,
      "loss": 1.7005,
      "step": 440
    },
    {
      "epoch": 0.4887320119467825,
      "grad_norm": 6.373684406280518,
      "learning_rate": 4.49e-05,
      "loss": 1.6599,
      "step": 450
    },
    {
      "epoch": 0.4995927233233777,
      "grad_norm": 4.056774139404297,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 1.7104,
      "step": 460
    },
    {
      "epoch": 0.5104534346999728,
      "grad_norm": 3.538161516189575,
      "learning_rate": 4.69e-05,
      "loss": 1.6713,
      "step": 470
    },
    {
      "epoch": 0.521314146076568,
      "grad_norm": 72.58341217041016,
      "learning_rate": 4.79e-05,
      "loss": 1.64,
      "step": 480
    },
    {
      "epoch": 0.5321748574531632,
      "grad_norm": 3.8803558349609375,
      "learning_rate": 4.89e-05,
      "loss": 1.6601,
      "step": 490
    },
    {
      "epoch": 0.5430355688297583,
      "grad_norm": 4.668607234954834,
      "learning_rate": 4.99e-05,
      "loss": 1.6723,
      "step": 500
    },
    {
      "epoch": 0.5430355688297583,
      "eval_loss": 1.4804284572601318,
      "eval_runtime": 58.5425,
      "eval_samples_per_second": 13.973,
      "eval_steps_per_second": 13.973,
      "step": 500
    },
    {
      "epoch": 0.5538962802063535,
      "grad_norm": 3.931116819381714,
      "learning_rate": 4.89311163895487e-05,
      "loss": 1.6591,
      "step": 510
    },
    {
      "epoch": 0.5647569915829487,
      "grad_norm": 4.13977575302124,
      "learning_rate": 4.7743467933491685e-05,
      "loss": 1.6686,
      "step": 520
    },
    {
      "epoch": 0.5756177029595438,
      "grad_norm": 4.645842552185059,
      "learning_rate": 4.655581947743468e-05,
      "loss": 1.702,
      "step": 530
    },
    {
      "epoch": 0.586478414336139,
      "grad_norm": 3.525282859802246,
      "learning_rate": 4.536817102137767e-05,
      "loss": 1.5935,
      "step": 540
    },
    {
      "epoch": 0.5973391257127342,
      "grad_norm": 4.945929527282715,
      "learning_rate": 4.418052256532067e-05,
      "loss": 1.6758,
      "step": 550
    },
    {
      "epoch": 0.6081998370893293,
      "grad_norm": 3.8379905223846436,
      "learning_rate": 4.299287410926366e-05,
      "loss": 1.7171,
      "step": 560
    },
    {
      "epoch": 0.6190605484659245,
      "grad_norm": 3.9524643421173096,
      "learning_rate": 4.1805225653206655e-05,
      "loss": 1.6819,
      "step": 570
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 3.9997856616973877,
      "learning_rate": 4.061757719714965e-05,
      "loss": 1.6124,
      "step": 580
    },
    {
      "epoch": 0.6407819712191148,
      "grad_norm": 3.850907802581787,
      "learning_rate": 3.9429928741092636e-05,
      "loss": 1.5604,
      "step": 590
    },
    {
      "epoch": 0.65164268259571,
      "grad_norm": 4.816882133483887,
      "learning_rate": 3.824228028503563e-05,
      "loss": 1.6745,
      "step": 600
    },
    {
      "epoch": 0.6625033939723052,
      "grad_norm": 4.450757026672363,
      "learning_rate": 3.7054631828978624e-05,
      "loss": 1.5528,
      "step": 610
    },
    {
      "epoch": 0.6733641053489003,
      "grad_norm": 4.243895053863525,
      "learning_rate": 3.586698337292162e-05,
      "loss": 1.6405,
      "step": 620
    },
    {
      "epoch": 0.6842248167254955,
      "grad_norm": 3.559950351715088,
      "learning_rate": 3.467933491686461e-05,
      "loss": 1.629,
      "step": 630
    },
    {
      "epoch": 0.6950855281020907,
      "grad_norm": 4.597898006439209,
      "learning_rate": 3.3491686460807606e-05,
      "loss": 1.648,
      "step": 640
    },
    {
      "epoch": 0.7059462394786858,
      "grad_norm": 68.7878646850586,
      "learning_rate": 3.23040380047506e-05,
      "loss": 1.5226,
      "step": 650
    },
    {
      "epoch": 0.716806950855281,
      "grad_norm": 3.826624631881714,
      "learning_rate": 3.111638954869359e-05,
      "loss": 1.5317,
      "step": 660
    },
    {
      "epoch": 0.7276676622318762,
      "grad_norm": 4.150129318237305,
      "learning_rate": 2.992874109263658e-05,
      "loss": 1.5943,
      "step": 670
    },
    {
      "epoch": 0.7385283736084713,
      "grad_norm": 5.196499347686768,
      "learning_rate": 2.8741092636579575e-05,
      "loss": 1.5885,
      "step": 680
    },
    {
      "epoch": 0.7493890849850665,
      "grad_norm": 5.782687187194824,
      "learning_rate": 2.7553444180522565e-05,
      "loss": 1.5731,
      "step": 690
    },
    {
      "epoch": 0.7602497963616617,
      "grad_norm": 4.028837203979492,
      "learning_rate": 2.636579572446556e-05,
      "loss": 1.6141,
      "step": 700
    },
    {
      "epoch": 0.7711105077382568,
      "grad_norm": 3.477818012237549,
      "learning_rate": 2.5178147268408553e-05,
      "loss": 1.6099,
      "step": 710
    },
    {
      "epoch": 0.781971219114852,
      "grad_norm": 4.8626909255981445,
      "learning_rate": 2.3990498812351544e-05,
      "loss": 1.5627,
      "step": 720
    },
    {
      "epoch": 0.7928319304914472,
      "grad_norm": 2.681428909301758,
      "learning_rate": 2.2802850356294538e-05,
      "loss": 1.5298,
      "step": 730
    },
    {
      "epoch": 0.8036926418680423,
      "grad_norm": 8.303633689880371,
      "learning_rate": 2.161520190023753e-05,
      "loss": 1.6663,
      "step": 740
    },
    {
      "epoch": 0.8145533532446375,
      "grad_norm": 5.614598274230957,
      "learning_rate": 2.0427553444180522e-05,
      "loss": 1.5647,
      "step": 750
    },
    {
      "epoch": 0.8254140646212327,
      "grad_norm": 4.060365676879883,
      "learning_rate": 1.9239904988123516e-05,
      "loss": 1.5973,
      "step": 760
    },
    {
      "epoch": 0.8362747759978278,
      "grad_norm": 3.1768171787261963,
      "learning_rate": 1.8052256532066507e-05,
      "loss": 1.5492,
      "step": 770
    },
    {
      "epoch": 0.847135487374423,
      "grad_norm": 4.551910400390625,
      "learning_rate": 1.6864608076009504e-05,
      "loss": 1.5406,
      "step": 780
    },
    {
      "epoch": 0.8579961987510182,
      "grad_norm": 3.444204568862915,
      "learning_rate": 1.5676959619952495e-05,
      "loss": 1.5485,
      "step": 790
    },
    {
      "epoch": 0.8688569101276133,
      "grad_norm": 3.9971203804016113,
      "learning_rate": 1.4489311163895489e-05,
      "loss": 1.642,
      "step": 800
    },
    {
      "epoch": 0.8797176215042085,
      "grad_norm": 3.8147788047790527,
      "learning_rate": 1.3301662707838481e-05,
      "loss": 1.5545,
      "step": 810
    },
    {
      "epoch": 0.8905783328808037,
      "grad_norm": 6.859287261962891,
      "learning_rate": 1.2114014251781473e-05,
      "loss": 1.6031,
      "step": 820
    },
    {
      "epoch": 0.9014390442573988,
      "grad_norm": 4.423483371734619,
      "learning_rate": 1.0926365795724467e-05,
      "loss": 1.6339,
      "step": 830
    },
    {
      "epoch": 0.912299755633994,
      "grad_norm": 4.299580097198486,
      "learning_rate": 9.73871733966746e-06,
      "loss": 1.5393,
      "step": 840
    },
    {
      "epoch": 0.9231604670105892,
      "grad_norm": 3.8454244136810303,
      "learning_rate": 8.551068883610452e-06,
      "loss": 1.5407,
      "step": 850
    },
    {
      "epoch": 0.9340211783871843,
      "grad_norm": 4.396700859069824,
      "learning_rate": 7.363420427553444e-06,
      "loss": 1.5604,
      "step": 860
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 5.453821182250977,
      "learning_rate": 6.175771971496437e-06,
      "loss": 1.5628,
      "step": 870
    },
    {
      "epoch": 0.9557426011403747,
      "grad_norm": 3.2046995162963867,
      "learning_rate": 4.98812351543943e-06,
      "loss": 1.5371,
      "step": 880
    },
    {
      "epoch": 0.9666033125169698,
      "grad_norm": 4.011597156524658,
      "learning_rate": 3.8004750593824232e-06,
      "loss": 1.5239,
      "step": 890
    },
    {
      "epoch": 0.977464023893565,
      "grad_norm": 3.7937073707580566,
      "learning_rate": 2.612826603325416e-06,
      "loss": 1.5483,
      "step": 900
    },
    {
      "epoch": 0.9883247352701602,
      "grad_norm": 4.4474616050720215,
      "learning_rate": 1.4251781472684086e-06,
      "loss": 1.5271,
      "step": 910
    },
    {
      "epoch": 0.9991854466467553,
      "grad_norm": 3.4617557525634766,
      "learning_rate": 2.3752969121140145e-07,
      "loss": 1.5897,
      "step": 920
    }
  ],
  "logging_steps": 10,
  "max_steps": 921,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5531718781673472.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
