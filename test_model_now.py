from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch

# Charger le modÃ¨le
model_path = "artifacts/model_trainer/pegasus-samsum-model"
tokenizer_path = "artifacts/model_trainer/tokenizer"

print("ğŸ“¥ Chargement du modÃ¨le...")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
print(f"âœ… ModÃ¨le chargÃ© sur {device}!\n")

# Test 1: Conversation simple
dialogue1 = """
Amanda: I baked cookies. Do you want some?
Jerry: Sure!
Amanda: I'll bring you tomorrow :-)
"""

# Test 2: Conversation professionnelle
dialogue2 = """
John: Hey Sarah, did you finish the project report?
Sarah: Yes! I just sent it to you. Did you review the budget section?
John: I did. The numbers look good. Should we schedule a meeting with the team?
Sarah: Great idea. How about tomorrow at 2 PM?
John: Perfect. I'll send the calendar invite.
"""

def test_summary(dialogue, title):
    print("=" * 70)
    print(f"ğŸ§ª TEST: {title}")
    print("=" * 70)
    print("DIALOGUE:")
    print(dialogue)
    print("\nğŸ¤– GÃ‰NÃ‰RATION DU RÃ‰SUMÃ‰...\n")
    
    inputs = tokenizer(dialogue, max_length=1024, truncation=True, return_tensors="pt").to(device)
    summary_ids = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_length=128,
        num_beams=8,
        length_penalty=0.8
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    
    print("ğŸ“ RÃ‰SUMÃ‰ GÃ‰NÃ‰RÃ‰:")
    print(summary)
    print("=" * 70)
    print()

# Tester
test_summary(dialogue1, "Conversation simple - Cookies")
test_summary(dialogue2, "Conversation professionnelle - RÃ©union")

print("\nğŸ’¡ Si les rÃ©sumÃ©s ont du sens, votre modÃ¨le fonctionne bien!")
print("ğŸ’¡ Les scores ROUGE bas sont probablement dus au petit nombre d'exemples d'Ã©valuation.")